<header id="jumbo" class="grid">
  <div class="backimg"></div>
  <div class="content-jumbo">
    <h1>Image Classification with Transfer Learning</h1>
    <p>Coding Assignment #4 - Spring 2018</p>
    <a class="btn" [routerLink]="['/']">Go Back</a>
  </div>
</header>
<div class="container">
  <section class="row">
    <div class="col-md-12 col-sm-12">
      <h3>Introduction</h3>
      <p>
        This final project objective is to use the concept of deep learning to
        classify the content of images. Object classification has been largely
        improved recently with the growing availability of computational and
        space capacity via the cloud. Neural Network can be used to their full
        capabilities to understand and classify images content. <br /><br />
        In this project, we use a Convolutional Neural Network to classify to
        type of content in images: Healthy Cocoa Pod and Sick Cocoa Pod.
      </p>
    </div>

    <div class="cocoaEx col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/healthyPod.png" alt="healthyPod" />
      <img src="../../../assets/images/cocoa/sickPod.png" alt="sickPod" />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="col-md-12 col-sm-12">
      <h3>Motivations</h3>
      <p>
        The current boom of Deep learning has enable a lot of projects in a
        various range of subjects and disciplines. A lot of codes, tutorials and
        forums are available online to help in the building of classification
        projects. In terms of Computer Vision, Deep Learning algorithm has
        improved by far understanding of images content.

        <br /><br />
        Having previously earned a master’s degree in agriculture, it always has
        been my interest to merge the two fields of Agriculture and Computer
        Science in the same long-term project. Moreover, my previous experiences
        in West Africa specifically in Ghana, has confronted me with numerous
        problems in the agriculture sector. Agriculture represents the largest
        employer and contributor to the GPD in the West African region. In
        particular, cocoa production is one of the main source of revenue within
        the agriculture sector. However, in the recent years, producers have
        been confronted with an increase of diseases which has led to
        significant loss in production. Unlike other cash crops, cocoa diseases
        often require total alienation of cocoa trees. As a consequence, farmers
        lose substantial capital. Thus, early detection of diseases is
        primordial in the management of plantation to minimize farmers losses.

        <br /><br />
        Project like this one could bring an effective and cheap detection tool
        to local farmer via the use of smartphones (which numbers are booming on
        the continent) and could have a real impact of cocoa production
        efficiency.
        <br />
      </p>
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="dataset col-md-12 col-sm-12">
      <h3>Datasets</h3>
      <p style="text-align: left">
        In order to run a deep learning algorithm on images, we need images, a
        lot of images. Within the given time and the topic of the projects, it
        has not been possible to find a real and consequent database of cocoa
        pods and we have not been able to collect real images from the fields.
        All images used in this project have been found via three main sources:
        Google Images, Flicker and
        <a href="http://www.image-net.org/" target="_blank">ImageNet database</a
        >. <br /><br />
        <li>
          IMAGENET <br />
          After a bit of research, it appeared that the ImageNet Database had
          around 700 of classified cocoa pods. However, there were barely any
          sick or diseased pods in the associated folder (and if any, they were
          all mixed up).
        </li>
        <br />
        <li>
          GOOGLE IMAGE / FLICKER SCRAPPING <br />
          We used some python code to scrapped images from Google Images and
          Flicker to collect the rest of the dataset. Many images collected were
          irrelevant or duplicates and a lot of cleaning had to be done in order
          to obtain a correct dataset.
        </li>
      </p>
    </div>

    <div class="datasetImg col-md-12 col-sm-12">
      <img
        src="../../../assets/images/cocoa/healthyPodsDataset.png"
        alt="healthyPodsDataset"
      />
    </div>
    <div class="dataset col-md-12 col-sm-12">
      <p>
        However, the set of collected sick cocoa pods images was quite thin.
        Some cropping and duplication using Paint have been done to enlarge the
        number of images. For example, when two pods were found in one image,
        each pod was cropped and saved as its own image.
        <br /><br />
        In the end, our images have been separated in two folders: Healthy and
        Sick. A total of 1,400 images have been collected for the Healthy
        category and a bit more than 700 images have been found for the Sick
        category.
      </p>
    </div>
    <div class="datasetImg col-md-12 col-sm-12">
      <img
        src="../../../assets/images/cocoa/sickPodsDataset.png"
        alt="sickPodsDataset"
      />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h3>Challenges</h3>
      <p style="text-align: left">
        <li>
          DISEASES DIVERSITY <br />The first difficulty is the numerous types of
          diseases which can affect cocoa pods and cocoa tree: Black Pod,
          Witches Broom, Frosty Pod Rot, Swollen Shoot Virus, etc. Images found
          of sick pods or sick trees have not been well documented and without
          expertise classification of sickness could not be done as this step of
          the project. Furthermore, in order to obtain good results in Deep
          Learning project, it is primordial to have a very large number of
          pictures available. Our 1400 and 700 images (for healthy and sick
          category respectively), are definitely not enough to run a
          Convolutional Neural Network from scratch. Therefore, it was
          impossible for this project to classify in detail the type of disease
          involved in the pictures. We reduced thus our project to a binary
          classification: healthy/sick.
        </li>
        <br />
        <li>
          SMALL DATASETS <br />Subsequently, due to a rather quite small
          dataset, our Deep Learning model has a very high probability of
          overfitting the results to our training set. Fortunately, techniques
          exist to our advantage (see methodology) to avoid or at least reduce
          the overfitting problem.
        </li>
        <br />
        <li>
          SMARTPHONE APPLICATION<br />Lastly, the ultimate challenge regards the
          final use of the model via smartphone. The end goal is to produce a
          smartphone application which can correctly classifies healthy and sick
          pods. The use of smartphones brings two limitations to the model:
          first its need to be relatively small in terms of memory and
          computation (during prediction) to be able to run on a phone in just a
          few seconds without freezing the rest of the device. Secondly, the
          model must be able to classify on low resolutions images taken from
          the device. Those two limitations have not been looked at in this
          current project but can maybe be overcome in the future by using
          technologies like Android Tensorflow API for the memory and
          computational issue and by training our model on lower resolutions
          images in the future.
        </li>
      </p>
    </div>
  </section>
  <hr class="style1" />
  <section class="row ">
    <div class="col-md-12 col-sm-12 ">
      <h3>Methodology</h3>
      <br />
      <h5 style="text-align: left">CONVOLUTIONAL NEURAL NETWORK</h5>
      <p style="text-align: left">
        Convolutional Neural Networks (ConvNets or CNNs) are a category of
        Neural Networks that have been very effective in image recognition and
        classification. ConvNets have been successful in identifying faces,
        objects and traffic signs. ConvNets derive their name from the
        “convolution” operator. The primary goal of a ConvNet is to extract
        features from the input image using convolution which preserves the
        spatial relationship between pixels by learning image features from
        small squares of input data.
      </p>
    </div>
    <div class="col-md-12 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/CNN.png"
        alt="CNN "
      />
    </div>
    <div class="col-md-12 col-sm-12 ">
      <p style="text-align: left">
        A ConvNet is mainly composed of four main operations:
        <li>Convolution</li>
        <li>Non-Linearity (ReLU)</li>
        <li>Pooling</li>
        <li>Classification i.e. Fully Connected</li>
        <br />
        For each layer in a convolution, a filter slides over the input image to
        produce a feature map. Thus, the convolution operation captures the
        local dependencies in the original image. A CNN learns the values of
        many different filters on its own during the training process. However,
        the number of filters, the filter size and type, and the architecture of
        the network need to be chosen before training. The more number of
        filters we have, the more image features get extracted and the better
        the network becomes at recognizing patterns in unseen images.
        <br /><br />
        The next operation is called ReLU for Rectified Linear Unit. It’s a
        non-linear, element wise operation which replaces all negative pixel
        values by zero, and keep the positives values as is. Other non-linear
        functions such as tanh or sigmoid can also be used instead of ReLU, but
        ReLU has been found to perform better in most situations.
        <br /><br />
        The third operation is pooling (also called downsampling). It reduces
        the dimensionality of each feature map but retains the most important
        information. Different type of pooling can be used, like Max, Average,
        Sum, etc. but Max Pooling has been shown to work the best. Pooling have
        several uses. First, it makes the input representations smaller and more
        manageable. Second, it reduces the number of parameters and computations
        in the network, thus reducing overfitting. Finally, it makes the network
        invariant to small transformations, distortions and translations in the
        input image.
        <br /><br />
        The last operation is the Fully Connected Layer. It uses a softmax
        activation function in the output later and has as many neurons as in
        the previous layer. This fully connected layer use high level features
        extracted by the previous steps (operations) to classify the input image
        into on the category or classes defined by the training set. The overall
        training process of the ConvNet may be summarized as follow:
        <br /><br />
        <li>
          <b>STEP 1</b>: Initialize all filters and parameters / weights with
          random values
        </li>
        <br />
        <li>
          <b>STEP 2</b>: The network takes a training image as input, goes
          through the forward propagation step (convolution, ReLU, pooling
          operations and forward propagation in the Fully Connected layer) and
          finds the output probabilities for each class.
        </li>
        <br />
        <li>
          <b>STEP 3</b>: Calculate the total error at the output layer. Total
          Error = ∑ ½ (target probability – output probability) ²
        </li>
        <br />
        <li>
          <b>STEP 4</b>: Use Backpropagation to calculate the gradients of the
          error with respect to all weights in the network and use gradient
          descent to update all filter values / weights and parameter values to
          minimize the output error. The weights are adjusted in proportion to
          their contribution to the total error. This means that the network has
          learnt to classify this particular image correctly by adjusting its
          weights / filters such that the output error is reduced.
        </li>
        <br />
        <li>
          <b>STEP 5</b>: Repeat steps 2-4 with all images in the training set.
        </li>
        <br />
        The above steps train the ConvNet i.e. all the weights and parameters of
        the ConvNet have been optimized to correctly classify images from the
        training set. Thus, when a new (unseen) image is input into the ConvNet,
        the network would go through the forward propagation step and output a
        probability for each. If the training set was large enough, the network
        will generalize well to new images and classify them into correct
        categories.
      </p>
    </div>
  </section>
  <hr class="style0" />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h5 style="text-align: left">VGG 16</h5>
      <p style="text-align: left">
        VGG is a convolutional neural network model proposed by K. Simonyan and
        A. Zisserman from the University of Oxford in the paper “Very Deep
        Convolutional Networks for Large-Scale Image Recognition”. VGG-16 was
        trained on more than a million images and can classify images into 1000
        object categories, for example: keyboard, mouse, pencil, and many
        animals. As a result, the model has learned rich feature representations
        for a wide range of images. On the ImageNet dataset, which is a dataset
        of over 14 million images belonging to 1000 classes, VGG16 achieves a
        92.7% test accuracy.
        <br /><br />
        The VGG16 architecture is composed of 16 hidden layers (convolution with
        ReLU and max pooling), 3 fully connected layers with ReLU and final
        softmax output layer. This model is going to be the base of our project
        via transfer learning.
      </p>
    </div>
    <div class="col-md-10 col-sm-12 " style="padding-bottom: 30px">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/VGG16.png "
        alt="VGG16 "
      />
    </div>
  </section>
  <hr class="style0" />
  <section class="row">
    <div class="col-md-7 col-sm-12 ">
      <h5 style="text-align: left">TRANSFER LEARNING</h5>
      <p style="text-align: left">
        In a lot of real-world use cases, small-scale data collection can be
        extremely expensive or sometimes near-impossible (e.g. in medical
        imaging). Thus, building a classifier from small dataset is a
        challenging problem, but it is also a realistic one.
        <br /><br />
        Deep learning requires the ability to learn features automatically from
        the data, which is generally only possible when lots of training data is
        available --especially for problems where the input samples are very
        high-dimensional, like images. However, convolutional neural networks
        are by design one of the best models available for most "perceptual"
        problems (such as image classification), even with very little data to
        learn from. Moreover, CNN models are highly reusable. You can take an
        image classification trained on a large-scale dataset then reuse it on a
        significantly different problem with only minor changes. Specifically,
        in the case of computer vision, many pre-trained models (usually trained
        on the ImageNet dataset) are now publicly available for download and can
        be used to create models out of very little data. For example, a model
        trained on a large dataset of bird images will contain learned features
        like edges or horizontal lines that you can transfer to another dataset.
        <br /><br />
        Like mentioned earlier, we used the VGG16 architecture, which was
        pre-trained on the ImageNet dataset. Because the ImageNet dataset
        contains one class cocoa among its total of 1000 classes, this model
        already has learned some features that are relevant to our project.
        <br /><br />
        We only instantiated the convolutional part of the model i.e. everything
        up to the fully-connected layers. Then we run the model on our training,
        validation and test dataset once, recording the output as bottleneck
        features. Then we trained a small fully-connected model on top of the
        stored features (for training and validation sets). We stored the
        features offline rather than adding our fully-connected model directly
        on top of a frozen convolutional base and running the whole thing
        because of computational efficiency. Running VGG16 is expensive,
        especially on a CPU, and we want to only do it once.
      </p>
    </div>
    <div class="col-md-4 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/transferLearning.png "
        alt="transferLearning "
      />
    </div>
  </section>
  <hr class="style0" />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h5 style="text-align: left">DATA AUGMENTATION</h5>
      <p style="text-align: left">
        Running the Convolutional Neural Network from scratch requires many
        images. Unfortunately, our dataset is not consequent enough to provide
        correct results for our classification (as a matter of fact, a model
        trained from nothing provides results based on chance – 1 over 2 – not
        better than 50% accuracy). Even while using transfer learning, the small
        number of images available (mostly for the sick category) may increase
        the risk of overfitting for our model. In order to alleviate this issue,
        data augmentation has been performed on the two datasets. Data
        augmentation consists in using the set of images we already have and
        transform them via different techniques (shear, noising, flip, zoom,
        rescale, etc.) to different images. It has been proven that data
        augmentation can effectively improve neural network results since the
        newly created images are not exactly similar to the originals and
        consequently enlarge the range of diversity in the dataset and thus the
        learning of the model.
        <br /><br />
        For the Sick category, aggressive data augmentation has been applied to
        the images:
        <li>Blurring</li>
        <li>Horizontal flip</li>
        <li>65 degrees rotation</li>
        <br />
        The images generation consequently brings the number of sick pods images
        to 2,800.
        <br /><br />
        For the Healthy category, only one type of data augmentation has been
        applied in order to get the same number of final images as in the Sick
        category (around 2,800):
        <li>Blurring</li>
      </p>
    </div>
    <div class="col-md-12 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/dataAugmentation.png "
        alt="dataAugmentation "
      />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h3>Results</h3>
      <p style="text-align: left">
        Running a CNN project requires a lot of fine tuning of parameters. Thus,
        several tests have been run to obtain the best accuracy on the
        validation set and the best predictions on test data.
        <br /><br />
      </p>
    </div>
    <div class="col-md-8 col-sm-12 ">
      <p style="text-align: left">
        <li>
          RMSPROP OPTIMIZER - DROPOUT 0.7<br /><br />
          The third test consist to fine-tune the dropout ration while keeping
          the same optimizer (RMSProp) and mean scaling. Dropout is a method to
          regularize the model by “turning off” some neurons in every layer. It
          prevents a layer to see the exact same pattern twice, thus helping
          reduce overfitting. In the previous experiment, the dropout ration was
          set up at 0.5. In this test, we decided to change the ration of 0.7 in
          the first added Dense layer. After 50 epochs, we obtained an accuracy
          of 95 % on the training set, of globally 88 % of the validation set.
          We obtained a precision of 0.93, a recall 0f 0.92 and a F1 score of
          0.92. Thus, a higher dropout (from 0.5 to 0.7) does not have an impact
          on the prediction of the model. However, we can see both a difference
          between the training accuracy (close to 95%) and the cross-validation
          and test accuracy, which drop around 88%. It probably means that the
          current model is slightly overfitting to the training data.
        </li>
      </p>
    </div>
    <div class="datasetImg col-md-4 col-sm-12">
      <img
        src="../../../assets/images/cocoa/CMDropout07.png"
        alt="CMDropout07"
      />
    </div>
    <div class="col-md-8 col-sm-12 ">
      <p style="text-align: left">
        <li>
          RMSPROP OPTIMIZER - DROPOUT 0.9 - 50 Epochs<br /><br />
          Since an increase of the dropout ration increase the performance of
          the model, we decided to try with an even larger dropout ration of 0.9
          (the rest staying the same). We obtained an accuracy of 91.5 % in the
          training set, around 88-89 % in the validation set. Thus, the gap
          between the training set and validation set got reduced, which may
          indicate that our model may less overfit its prediction to the
          training set. On the test set, we obtain a precision of 0.95 and a
          recall of 0.91. In the end the F1 score is 0.93 which is slightly
          better than the previous test with a dropout of 0.7 or 0.5. Thus, it
          is right now our best model, with less overfitting and our best F1
          score.
        </li>
      </p>
    </div>
    <div class="datasetImg col-md-4 col-sm-12">
      <img
        src="../../../assets/images/cocoa/CMDropout09.png"
        alt="CMDropout09"
      />
    </div>
    <div class="col-md-8 col-sm-12 ">
      <p style="text-align: left">
        <li>
          RMSPROP OPTIMIZER - DROPOUT 0.9 - 100 Epochs<br /><br />
          Since the results are good, let’s try again the same model but with
          100 epochs instead of 50. We observed an increase of accuracy on the
          train set to 93%. However, the results on the cross-validation set are
          the same 88-89%, which show that with more epoch the model tens to
          overfitting a bit. The precision score is 0.95 like previously and the
          recall is 0.90, slightly worse than with 50 epochs. Thus, the F1 score
          is 0.92.
        </li>
      </p>
    </div>
    <div class="datasetImg col-md-4 col-sm-12">
      <img
        src="../../../assets/images/cocoa/CMDropout09Epoch100.png"
        alt="CMDropout09Epoch100"
      />
    </div>
    <div class="datasetImg col-md-4 col-sm-12">
      <img
        src="../../../assets/images/cocoa/LR03Epoch100.png"
        alt="LR03Epoch100"
      />
    </div>
    <div class="col-md-8 col-sm-12 ">
      <p style="text-align: left">
        <li>
          SGD OPTIMIZER – DROPOUT 0.5 - LEARNING RATE: 0.0003 – 100 Epochs<br /><br />
          For 100 epochs though, we obtained worse results. The accuracy on the
          training set stayed the same around 89%, and the cross validation, we
          reach once 90%, but on average we also got around 84 to 88 %. It seems
          the model is not overfitting but may have some issue to converge. We
          can see a net decrease in the correct prediction of healthy pod. In
          this case we got a precision score of 0f 0.79 and a recall of 0.95,
          which gives a F1 score of 0.86. Thus, a large decrease in the accuracy
          is observed with more epoch. This model is in consequent not the best
          one to use.
        </li>
      </p>
    </div>
    <div class="datasetImg col-md-4 col-sm-12">
      <img
        src="../../../assets/images/cocoa/LR00003Dropout05.png"
        alt="LR00003Dropout05"
      />
    </div>
    <div class="col-md-8 col-sm-12 ">
      <p style="text-align: left">
        <li>
          SGD OPTIMIZER – DROPOUT 0.5 - LEARNING RATE: 0.000003<br /><br />
          Back with a dropout of 0.5, we changed the learning rate to 0.000003.
          The train accuracy reached 96% with this model, which is less than the
          99% of 0.00003 learning rate. For the cross-validation, we got around
          86 %. Once again, the model is quite overfitting the data to the
          training set. Precision for this model is 0.94 and recall is 0.91
          which gives a F1 score of 0.92. Not our best model, not the worst, but
          however overfitting.
        </li>
      </p>
    </div>
    <div class="col-md-12 col-sm-12 ">
      <h5>SUMMARY</h5>
      <p style="text-align: left">
        The best model we have obtained so far is: RMSProp Optimizer – Dropout
        0.9. It obtained an F1 score of 0.93. The training accuracy is 93%, with
        a cross-validation accuracy of 88-89%, which shows that the model is not
        overfitting too much and can predict correctly for most of the data.
        However, a lot more of fine-tuning can be done in terms of optimizers
        choice and learning rates. As well, momentum, and decay have not been
        changed in the SGD optimizer. It would be also interested with more
        time, to test the models with more epochs to see if a better accuracy
        can be obtained. Although, each model has been trained only once which
        parameters. In the future, we should train several times the models and
        takes the average results for more consistent analysis. Also, we can
        observe that whatever the model looked at, the best accuracy observed on
        the cross-validation never went over the 89 %. Getting to the 90’s %
        would be a great improvement in the future. This probably can be
        reachable if more data can be collected.
      </p>
    </div>
    <div class="resultsImg col-md-12 col-sm-12" style="padding-bottom: 30px">
      <img
        src="../../../assets/images/cocoa/RMSPropResults.png"
        alt="RMSPropResults"
      />
    </div>
    <div class="resultsImg col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/SDGResults.png" alt="SDGResults" />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="examplesImg col-md-12 col-sm-12">
      <h3>Examples</h3>
      <p style="text-align: left">
        Here are some predictions made by our best model (RMSProp Optimizer –
        Dropout 0.9 – 100 epochs). Labels 1 correspond to sick predicted pods,
        labels 0 correspond to healthy predicted pods.
        <br /><br />
        In red are example of wrong predictions. Globally, classifications were
        mostly correct, even with rotated, shifted and blurred images. We can
        see though some misclassifications (like in the last examples), where
        the prediction was correct for the rotated image but not for the
        original and blurred pictures. Similarly, we can see one example where
        the blurred image was misclassified but not the original.
      </p>
    </div>
    <div class="examplesImg col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/examples1.png" alt="example1" />
      <img src="../../../assets/images/cocoa/examples2.png" alt="example2" />
      <img src="../../../assets/images/cocoa/examples3.png" alt="example4" />
      <img src="../../../assets/images/cocoa/examples4.png" alt="example4" />
    </div>
    <div class="examplesImg col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/examples5.png" alt="example5" />
      <img src="../../../assets/images/cocoa/examples6.png" alt="example6" />
      <img src="../../../assets/images/cocoa/examples7.png" alt="example7" />
      <img src="../../../assets/images/cocoa/examples8.png" alt="example8" />
    </div>
    <div class="examplesImg col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/examples9.png" alt="example9" />
      <img src="../../../assets/images/cocoa/examples10.png" alt="example10" />
      <img src="../../../assets/images/cocoa/examples11.png" alt="example11" />
      <img src="../../../assets/images/cocoa/examples12.png" alt="example12" />
    </div>
    <div class="examplesImg col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/examples13.png" alt="example13" />
    </div>
  </section>
</div>
