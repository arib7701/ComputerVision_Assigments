<header id="jumbo" class="grid">
  <div class="backimg"></div>
  <div class="content-jumbo">
    <h1>Image Classification with Transfer Learning</h1>
    <p>Coding Assignment #4 - Spring 2018</p>
    <a class="btn" [routerLink]="['/']">Go Back</a>
  </div>
</header>
<div class="container">
  <section class="row">
    <div class="col-md-12 col-sm-12">
      <h3>Introduction:</h3>
      <p>
        This final project objective is to use the concept of deep learning to
        classify the content of images. Object classification has been largely
        improved recently with the growing availability of computational and
        space capacity via the cloud. Neural Network can be used to their full
        capabilities to understand and classify images content. <br /><br />
        In this project, we use a Convolutional Neural Network to classify to
        type of content in images: Healthy Cocoa Pod and Sick Cocoa Pod.
      </p>
    </div>

    <div class="cocoaEx col-md-12 col-sm-12">
      <img src="../../../assets/images/cocoa/healthyPod.png" alt="healthyPod" />
      <img src="../../../assets/images/cocoa/sickPod.png" alt="sickPod" />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="col-md-12 col-sm-12">
      <h3>Motivations:</h3>
      <p>
        The current boom of Deep learning has enable a lot of projects in a
        various range of subjects and disciplines. A lot of codes, tutorials and
        forums are available online to help in the building of classification
        projects. In terms of Computer Vision, Deep Learning algorithm has
        improved by far understanding of images content.

        <br /><br />
        Having previously earned a master’s degree in agriculture, it always has
        been my interest to merge the two fields of Agriculture and Computer
        Science in the same long-term project. Moreover, my previous experiences
        in West Africa specifically in Ghana, has confronted me with numerous
        problems in the agriculture sector. Agriculture represents the largest
        employer and contributor to the GPD in the West African region. In
        particular, cocoa production is one of the main source of revenue within
        the agriculture sector. However, in the recent years, producers have
        been confronted with an increase of diseases which has led to
        significant loss in production. Unlike other cash crops, cocoa diseases
        often require total alienation of cocoa trees. As a consequence, farmers
        lose substantial capital. Thus, early detection of diseases is
        primordial in the management of plantation to minimize farmers losses.

        <br /><br />
        Project like this one could bring an effective and cheap detection tool
        to local farmer via the use of smartphones (which numbers are booming on
        the continent) and could have a real impact of cocoa production
        efficiency.
        <br />
      </p>
    </div>
  </section>
  <hr />
  <section class="row">
    <div class="dataset col-md-12 col-sm-12">
      <h4>Datasets</h4>
      <p style="text-align: left">
        In order to run a deep learning algorithm on images, we need images, a
        lot of images. Within the given time and the topic of the projects, it
        has not been possible to find a real and consequent database of cocoa
        pods and we have not been able to collect real images from the fields.
        All images used in this project have been found via three main sources:
        Google Images, Flicker and
        <a href="http://www.image-net.org/" target="_blank">ImageNet database</a
        >. <br /><br />
        <li>
          IMAGENET <br />
          After a bit of research, it appeared that the ImageNet Database had
          around 700 of classified cocoa pods. However, there were barely any
          sick or diseased pods in the associated folder (and if any, they were
          all mixed up).
        </li>
        <br />
        <li>
          GOOGLE IMAGE / FLICKER SCRAPPING <br />
          We used some python code to scrapped images from Google Images and
          Flicker to collect the rest of the dataset. Many images collected were
          irrelevant or duplicates and a lot of cleaning had to be done in order
          to obtain a correct dataset.
        </li>
      </p>
    </div>

    <div class="datasetImg col-md-12 col-sm-12">
      <img
        src="../../../assets/images/cocoa/healthyPodsDataset.png"
        alt="healthyPodsDataset"
      />
    </div>
    <div class="dataset col-md-12 col-sm-12">
      <p>
        However, the set of collected sick cocoa pods images was quite thin.
        Some cropping and duplication using Paint have been done to enlarge the
        number of images. For example, when two pods were found in one image,
        each pod was cropped and saved as its own image.
        <br /><br />
        In the end, our images have been separated in two folders: Healthy and
        Sick. A total of 1,400 images have been collected for the Healthy
        category and a bit more than 700 images have been found for the Sick
        category.
      </p>
    </div>
    <div class="datasetImg col-md-12 col-sm-12">
      <img
        src="../../../assets/images/cocoa/sickPodsDataset.png"
        alt="sickPodsDataset"
      />
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h4>Challenges</h4>
      <p style="text-align: left">
        <li>
          DISEASES DIVERSITY <br />The first difficulty is the numerous types of
          diseases which can affect cocoa pods and cocoa tree: Black Pod,
          Witches Broom, Frosty Pod Rot, Swollen Shoot Virus, etc. Images found
          of sick pods or sick trees have not been well documented and without
          expertise classification of sickness could not be done as this step of
          the project. Furthermore, in order to obtain good results in Deep
          Learning project, it is primordial to have a very large number of
          pictures available. Our 1400 and 700 images (for healthy and sick
          category respectively), are definitely not enough to run a
          Convolutional Neural Network from scratch. Therefore, it was
          impossible for this project to classify in detail the type of disease
          involved in the pictures. We reduced thus our project to a binary
          classification: healthy/sick.
        </li>
        <br />
        <li>
          SMALL DATASETS <br />Subsequently, due to a rather quite small
          dataset, our Deep Learning model has a very high probability of
          overfitting the results to our training set. Fortunately, techniques
          exist to our advantage (see methodology) to avoid or at least reduce
          the overfitting problem.
        </li>
        <br />
        <li>
          SMARTPHONE APPLICATION<br />Lastly, the ultimate challenge regards the
          final use of the model via smartphone. The end goal is to produce a
          smartphone application which can correctly classifies healthy and sick
          pods. The use of smartphones brings two limitations to the model:
          first its need to be relatively small in terms of memory and
          computation (during prediction) to be able to run on a phone in just a
          few seconds without freezing the rest of the device. Secondly, the
          model must be able to classify on low resolutions images taken from
          the device. Those two limitations have not been looked at in this
          current project but can maybe be overcome in the future by using
          technologies like Android Tensorflow API for the memory and
          computational issue and by training our model on lower resolutions
          images in the future.
        </li>
      </p>
    </div>
  </section>
  <hr class="style1" />
  <section class="row ">
    <div class="col-md-12 col-sm-12 ">
      <h4>Methodology</h4>
      <br />
      <h5 style="text-align: left">CONVOLUTIONAL NEURAL NETWORK</h5>
      <p style="text-align: left">
        Convolutional Neural Networks (ConvNets or CNNs) are a category of
        Neural Networks that have been very effective in image recognition and
        classification. ConvNets have been successful in identifying faces,
        objects and traffic signs. ConvNets derive their name from the
        “convolution” operator. The primary goal of a ConvNet is to extract
        features from the input image using convolution which preserves the
        spatial relationship between pixels by learning image features from
        small squares of input data.
      </p>
    </div>
    <div class="col-md-12 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/CNN.png"
        alt="CNN "
      />
    </div>
    <div class="col-md-12 col-sm-12 ">
      <p style="text-align: left">
        A ConvNet is mainly composed of four main operations:
        <li>Convolution</li>
        <li>Non-Linearity (ReLU)</li>
        <li>Pooling</li>
        <li>Classification i.e. Fully Connected</li>
        <br />
        For each layer in a convolution, a filter slides over the input image to
        produce a feature map. Thus, the convolution operation captures the
        local dependencies in the original image. A CNN learns the values of
        many different filters on its own during the training process. However,
        the number of filters, the filter size and type, and the architecture of
        the network need to be chosen before training. The more number of
        filters we have, the more image features get extracted and the better
        the network becomes at recognizing patterns in unseen images.
        <br /><br />
        The next operation is called ReLU for Rectified Linear Unit. It’s a
        non-linear, element wise operation which replaces all negative pixel
        values by zero, and keep the positives values as is. Other non-linear
        functions such as tanh or sigmoid can also be used instead of ReLU, but
        ReLU has been found to perform better in most situations.
        <br /><br />
        The third operation is pooling (also called downsampling). It reduces
        the dimensionality of each feature map but retains the most important
        information. Different type of pooling can be used, like Max, Average,
        Sum, etc. but Max Pooling has been shown to work the best. Pooling have
        several uses. First, it makes the input representations smaller and more
        manageable. Second, it reduces the number of parameters and computations
        in the network, thus reducing overfitting. Finally, it makes the network
        invariant to small transformations, distortions and translations in the
        input image.
        <br /><br />
        The last operation is the Fully Connected Layer. It uses a softmax
        activation function in the output later and has as many neurons as in
        the previous layer. This fully connected layer use high level features
        extracted by the previous steps (operations) to classify the input image
        into on the category or classes defined by the training set. The overall
        training process of the ConvNet may be summarized as follow:
        <br /><br />
        <li>
          <b>STEP 1</b>: Initialize all filters and parameters / weights with
          random values
        </li>
        <br />
        <li>
          <b>STEP 2</b>: The network takes a training image as input, goes
          through the forward propagation step (convolution, ReLU, pooling
          operations and forward propagation in the Fully Connected layer) and
          finds the output probabilities for each class.
        </li>
        <br />
        <li>
          <b>STEP 3</b>: Calculate the total error at the output layer. Total
          Error = ∑ ½ (target probability – output probability) ²
        </li>
        <br />
        <li>
          <b>STEP 4</b>: Use Backpropagation to calculate the gradients of the
          error with respect to all weights in the network and use gradient
          descent to update all filter values / weights and parameter values to
          minimize the output error. The weights are adjusted in proportion to
          their contribution to the total error. This means that the network has
          learnt to classify this particular image correctly by adjusting its
          weights / filters such that the output error is reduced.
        </li>
        <br />
        <li>
          <b>STEP 5</b>: Repeat steps 2-4 with all images in the training set.
        </li>
        <br />
        The above steps train the ConvNet i.e. all the weights and parameters of
        the ConvNet have been optimized to correctly classify images from the
        training set. Thus, when a new (unseen) image is input into the ConvNet,
        the network would go through the forward propagation step and output a
        probability for each. If the training set was large enough, the network
        will generalize well to new images and classify them into correct
        categories.
      </p>
    </div>
  </section>
  <hr />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h5 style="text-align: left">VGG 16</h5>
      <p style="text-align: left">
        VGG is a convolutional neural network model proposed by K. Simonyan and
        A. Zisserman from the University of Oxford in the paper “Very Deep
        Convolutional Networks for Large-Scale Image Recognition”. VGG-16 was
        trained on more than a million images and can classify images into 1000
        object categories, for example: keyboard, mouse, pencil, and many
        animals. As a result, the model has learned rich feature representations
        for a wide range of images. On the ImageNet dataset, which is a dataset
        of over 14 million images belonging to 1000 classes, VGG16 achieves a
        92.7% test accuracy.
        <br /><br />
        The VGG16 architecture is composed of 16 hidden layers (convolution with
        ReLU and max pooling), 3 fully connected layers with ReLU and final
        softmax output layer. This model is going to be the base of our project
        via transfer learning.
      </p>
    </div>
    <div class="col-md-10 col-sm-12 " style="padding-bottom: 30px">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/VGG16.png "
        alt="VGG16 "
      />
    </div>
  </section>
  <hr />
  <section class="row">
    <div class="col-md-7 col-sm-12 ">
      <h5 style="text-align: left">TRANSFER LEARNING</h5>
      <p style="text-align: left">
        In a lot of real-world use cases, small-scale data collection can be
        extremely expensive or sometimes near-impossible (e.g. in medical
        imaging). Thus, building a classifier from small dataset is a
        challenging problem, but it is also a realistic one.
        <br /><br />
        Deep learning requires the ability to learn features automatically from
        the data, which is generally only possible when lots of training data is
        available --especially for problems where the input samples are very
        high-dimensional, like images. However, convolutional neural networks
        are by design one of the best models available for most "perceptual"
        problems (such as image classification), even with very little data to
        learn from. Moreover, CNN models are highly reusable. You can take an
        image classification trained on a large-scale dataset then reuse it on a
        significantly different problem with only minor changes. Specifically,
        in the case of computer vision, many pre-trained models (usually trained
        on the ImageNet dataset) are now publicly available for download and can
        be used to create models out of very little data. For example, a model
        trained on a large dataset of bird images will contain learned features
        like edges or horizontal lines that you can transfer to another dataset.
        <br /><br />
        Like mentioned earlier, we used the VGG16 architecture, which was
        pre-trained on the ImageNet dataset. Because the ImageNet dataset
        contains one class cocoa among its total of 1000 classes, this model
        already has learned some features that are relevant to our project.
        <br /><br />
        We only instantiated the convolutional part of the model i.e. everything
        up to the fully-connected layers. Then we run the model on our training,
        validation and test dataset once, recording the output as bottleneck
        features. Then we trained a small fully-connected model on top of the
        stored features (for training and validation sets). We stored the
        features offline rather than adding our fully-connected model directly
        on top of a frozen convolutional base and running the whole thing
        because of computational efficiency. Running VGG16 is expensive,
        especially on a CPU, and we want to only do it once.
      </p>
    </div>
    <div class="col-md-4 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/transferLearning.png "
        alt="transferLearning "
      />
    </div>
  </section>
  <hr />
  <section class="row">
    <div class="col-md-12 col-sm-12 ">
      <h5 style="text-align: left">DATA AUGMENTATION</h5>
      <p style="text-align: left">
        Running the Convolutional Neural Network from scratch requires many
        images. Unfortunately, our dataset is not consequent enough to provide
        correct results for our classification (as a matter of fact, a model
        trained from nothing provides results based on chance – 1 over 2 – not
        better than 50% accuracy). Even while using transfer learning, the small
        number of images available (mostly for the sick category) may increase
        the risk of overfitting for our model. In order to alleviate this issue,
        data augmentation has been performed on the two datasets. Data
        augmentation consists in using the set of images we already have and
        transform them via different techniques (shear, noising, flip, zoom,
        rescale, etc.) to different images. It has been proven that data
        augmentation can effectively improve neural network results since the
        newly created images are not exactly similar to the originals and
        consequently enlarge the range of diversity in the dataset and thus the
        learning of the model.
        <br /><br />
        For the Sick category, aggressive data augmentation has been applied to
        the images:
        <li>Blurring</li>
        <li>Horizontal flip</li>
        <li>65 degrees rotation</li>
        <br />
        The images generation consequently brings the number of sick pods images
        to 2,800.
        <br /><br />
        For the Healthy category, only one type of data augmentation has been
        applied in order to get the same number of final images as in the Sick
        category (around 2,800):
        <li>Blurring</li>
      </p>
    </div>
    <div class="col-md-12 col-sm-12 ">
      <img
        id="downimg1 "
        src="../../../assets/images/cocoa/dataAugmentation.png "
        alt="dataAugmentation "
      />
    </div>
  </section>
  <hr class="style1" />
  <section class="row examples">
    <h4 class="col-md-12 col-sm-12">
      Others Examples (Original / KMeans / Mean Shift / Normalized Cut)
    </h4>
    <div class="col-md-12">
      <div class="segmentationEx col-md-12 col-sm-12">
        <img src="../../../assets/images/segmentation/lena.jpg" alt="lena" />
        <img
          src="../../../assets/images/segmentation/lena_KMeansSegmentation_10.jpg"
          alt="lena_Kmeans"
        />
        <img
          src="../../../assets/images/segmentation/lena200_MeanShift0.1Spatial.jpg"
          alt="lena_MeanShit"
        />
        <img
          src="../../../assets/images/segmentation/lena200_NCut.jpg"
          alt="lena_NCut"
        />
      </div>
      <div class="segmentationEx col-md-12 col-sm-12">
        <img src="../../../assets/images/segmentation/hand.jpg" alt="hand" />
        <img
          src="../../../assets/images/segmentation/hand200_KMeansSegmentation_6.jpg"
          alt="hand_Kmeans"
        />
        <img
          src="../../../assets/images/segmentation/hand200_MeanShift0.1Spatial.jpg"
          alt="hand_MeanShit"
        />
        <img
          src="../../../assets/images/segmentation/hand200_NCut.jpg"
          alt="hand_NCut"
        />
      </div>
      <div class="segmentationEx col-md-12 col-sm-12">
        <img
          src="../../../assets/images/segmentation/pilou300.jpg"
          alt="pilou"
        />
        <img
          src="../../../assets/images/segmentation/pilou250_KMeansSegmentation_6.jpg"
          alt="pilou_Kmeans"
        />
        <img
          src="../../../assets/images/segmentation/pilou250_MeanShift0.1Spatial.jpg"
          alt="pilou_MeanShit"
        />
        <img
          src="../../../assets/images/segmentation/pilou250_NCut.jpg"
          alt="pilou_NCut"
        />
      </div>
      <div class="segmentationEx col-md-12 col-sm-12">
        <img
          src="../../../assets/images/segmentation/Sighisoara.jpg"
          alt="Sighisoara"
        />
        <img
          src="../../../assets/images/segmentation/Sighisoara250_KMeansSegmentation_8.jpg"
          alt="sighisoara_Kmeans"
        />
        <img
          src="../../../assets/images/segmentation/Sighisoara250_MeanShift0.1Spatial.jpg"
          alt="sighisoara_MeanShit"
        />
        <img
          src="../../../assets/images/segmentation/Sighisoara250_NCut.jpg"
          alt="sighisoara_NCut"
        />
      </div>
      <div class="segmentationEx col-md-12 col-sm-12">
        <img
          src="../../../assets/images/segmentation/campus.jpg"
          alt="campus"
        />
        <img
          src="../../../assets/images/segmentation/campus200_KMeansSegmentation_6.jpg"
          alt="campus_Kmeans"
        />
        <img
          src="../../../assets/images/segmentation/campus200_MeanShift0.1Spatial.jpg"
          alt="campus_MeanShit"
        />
        <img
          src="../../../assets/images/segmentation/campus200_NCut.jpg"
          alt="campus_NCut"
        />
      </div>
    </div>
  </section>
  <hr class="style1" />
  <section class="row">
    <div class="concept col-md-12 col-sm-12">
      <h4>Pros & Cons</h4>
      <p>
        <b>K-Means:</b>
        <br />
        The main advantage of the K-means is that it’s a very simple method to
        implement and to understand. It also converges quite rapidly to local
        minimums. However, it does have some disadvantages. It requires a bit of
        memory since each pixel must be compared to each cluster at each
        iteration. As well, the user needs to pick the number of clusters as a
        parameter. Evaluating the number of cluster can be difficult for image
        segmentation. Also, the result will depend on the initialization of the
        cluster center. Different run of the algorithm will give different
        results since the centers are randomly chosen. The results will be
        depending on outliers too. Outliers will deform what could be defined as
        a normal cluster by a human eye. Finally, K-Means tends to only find
        spherical cluster and have a lot of difficulties with intertwined
        shapes. In the image experiments, the results show a good segmentation
        in all the case. However, the segmentation is sometimes too precise
        compared to some Mean-Shift results.
        <br />
        <br />
        <b>Normalized Cut:</b>
        <br />
        The main advantage of this method is that is based on a graph structure
        which have been well implemented. As well, the weight matrix can be
        computed in different way and may incorporate different types of
        features. It does not require a data distribution model. It has several
        disadvantages. The first one is the very high time and space complexity
        of the algorithm. For pictures of 512x512, the weight matrix will have a
        size of 262144x262144, which is quite important, mostly if the matrix is
        dense and not sparse. From that matrix, we have to find the eigenvalues,
        which proves to be quite a challenge for large matrix. It was the big
        issue in my implementation. As well, the algorithm when completed has a
        tendency to prefer balanced clusters in size, which can pause inaccuracy
        in the segmentation. Finally, the parameters of the implementation have
        a very high importance in the results. Some parameters will not allow
        the complete computation of the eigenvalues, and some parameters will
        give a very poor segmentation. In my image experiments, it was very
        difficult to find the right combinations of parameters to get good
        results. The results showed below are the best I could get and in my
        view are not as good as the K-Means and Mean-Shift results.
        <br />
        <br />
        <b>Mean Shift:</b>
        <br />
        The main advantage of the Mean Shift is that is find automatically the
        main peaks of attraction in the image. It also does not require more
        than one parameter (i.e. the window size). It’s a simple technique which
        can find multiple nodes in a relative small computational time. Also, it
        does not produce same size or specific shape on clusters. In the image
        experiments, the mean shift with spatial information produces in my
        opinion the best results. The clusters are more defined than in the
        Normalized Cut but are more averaging than with a K-Means
        implementation. The main disadvantage is that we still have to choose a
        parameter (the window size). Figuring the best window size can be
        difficult. As well, Mean-Shift does not scale well with high dimensions
        of the feature space.
      </p>
    </div>
  </section>
</div>
